---
layout: distill
title: Story of Linear Time Sequence Modeling ğŸ“š
description: Summary of Linear Transformers
tags:
giscus_comments: false
date: 2025-08-29
featured: false
thumbnail: assets/img/ourlogo.jpg

authors:
  - name: Arshia  
    url:
    affiliations:
      name: EPFL
  - name: Sajad 
    url:
    affiliations:
      name: ELISS
  - name: Timur
    url:
    affiliations:
      name: ELISS

bibliography: albert.bib


toc:
  - name: Sequence Modeling Core
  - name: From Causal to Full Linear Attention
  - name: Creating Scaled and Masked Full Attention
 
---



------------------

# Why Linear Transformers?

If youâ€™ve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, weâ€™d point you there first, theyâ€™ve done a fantastic job (honestly, better than us).

In this series, weâ€™ll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if youâ€™ve come across names like **Mamba** or **DeltaNet** and wondered what they are, or if youâ€™ve asked yourself: 

**1)** Why we moved from RNNs to Transformers?

**2)** Why we now seem to be circling back from Transformers toward Linear Transformers (almost like â€œRNNs on steroidsâ€)?
 
then this post should be a good fit ğŸ˜‰.

So letâ€™s start from answering the above questions 



----------------
