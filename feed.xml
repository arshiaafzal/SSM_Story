<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arshiaafzal.github.io/SSM_Story/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arshiaafzal.github.io/SSM_Story/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-31T10:10:51+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Story of Linear Time Sequence Modeling ğŸ“š</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling ğŸ“š"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If youâ€™ve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, weâ€™d point you there first, theyâ€™ve done a fantastic job (honestly, better than us).</p> <p>In this series, weâ€™ll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if youâ€™ve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if youâ€™ve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like â€œRNNs on steroidsâ€)?</p> <p>then this post should be a good fit ğŸ˜‰.</p> <p>So letâ€™s start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry><entry><title type="html">Story of Linear Time Sequence Modeling ğŸ“š</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling ğŸ“š"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If youâ€™ve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, weâ€™d point you there first, theyâ€™ve done a fantastic job (honestly, better than us).</p> <p>In this series, weâ€™ll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if youâ€™ve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if youâ€™ve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like â€œRNNs on steroidsâ€)?</p> <p>then this post should be a good fit ğŸ˜‰.</p> <p>So letâ€™s start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry><entry><title type="html">Story of Linear Time Sequence Modeling ğŸ“š</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling ğŸ“š"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If youâ€™ve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, weâ€™d point you there first, theyâ€™ve done a fantastic job (honestly, better than us).</p> <p>In this series, weâ€™ll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if youâ€™ve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if youâ€™ve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like â€œRNNs on steroidsâ€)?</p> <p>then this post should be a good fit ğŸ˜‰.</p> <p>So letâ€™s start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry><entry><title type="html">Story of Linear Time Sequence Modeling ğŸ“š</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling ğŸ“š"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If youâ€™ve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, weâ€™d point you there first, theyâ€™ve done a fantastic job (honestly, better than us).</p> <p>In this series, weâ€™ll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if youâ€™ve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if youâ€™ve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like â€œRNNs on steroidsâ€)?</p> <p>then this post should be a good fit ğŸ˜‰.</p> <p>So letâ€™s start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry></feed>