<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arshiaafzal.github.io/SSM_Story/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arshiaafzal.github.io/SSM_Story/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-31T09:58:46+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Story of Linear Time Sequence Modeling üìö</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling üìö"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part1-model/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If you‚Äôve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, we‚Äôd point you there first, they‚Äôve done a fantastic job (honestly, better than us).</p> <p>In this series, we‚Äôll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if you‚Äôve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if you‚Äôve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like ‚ÄúRNNs on steroids‚Äù)?</p> <p>then this post should be a good fit üòâ.</p> <p>So let‚Äôs start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry><entry><title type="html">Story of Linear Time Sequence Modeling üìö</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory/" rel="alternate" type="text/html" title="Story of Linear Time Sequence Modeling üìö"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part2-theory/"><![CDATA[<hr/> <h1 id="why-linear-transformers">Why Linear Transformers?</h1> <p>If you‚Äôve heard about Large Language Models (LLMs) and Transformers and are curious to learn more, there are already plenty of excellent blog posts, articles, and YouTube videos that explain them in great detail with amazing visualizations. In case your main interest is understanding LLMs and how Transformers work, we‚Äôd point you there first, they‚Äôve done a fantastic job (honestly, better than us).</p> <p>In this series, we‚Äôll cover Linear Transformers and State Space Models (SSMs), giving a high-level summary of their core ideas. So, if you‚Äôve come across names like <strong>Mamba</strong> or <strong>DeltaNet</strong> and wondered what they are, or if you‚Äôve asked yourself:</p> <p><strong>1)</strong> Why we moved from RNNs to Transformers?</p> <p><strong>2)</strong> Why we now seem to be circling back from Transformers toward Linear Transformers (almost like ‚ÄúRNNs on steroids‚Äù)?</p> <p>then this post should be a good fit üòâ.</p> <p>So let‚Äôs start from answering the above questions</p> <hr/>]]></content><author><name>Arshia</name></author><summary type="html"><![CDATA[Summary of Linear Transformers]]></summary></entry><entry><title type="html">LION ü¶Å Part III - Chunkwise Parallel from of LION</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk/" rel="alternate" type="text/html" title="LION ü¶Å Part III - Chunkwise Parallel from of LION"/><published>2025-02-28T00:00:00+00:00</published><updated>2025-02-28T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part3-chunk/"><![CDATA[<p>[<a href="https://www.arxiv.org/abs/2502.16249">Paper</a>] [<a href="https://github.com/LIONS-EPFL/LION">Code</a>]</p> <ol> <li><a href="/SSM_Story/2025/lion-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/SSM_Story/2025/lion-part2-theory/">Part II - Bi-directional RNN</a></li> <li>Part III - Chunkwise Parallel from of LION</li> <li><a href="/SSM_Story/2025/lion-part4-results/">Part IV - Results</a></li> </ol> <p>Since we have now established the LION framework, which maps Full Linear Attention into a bi-directional RNN in <a href="/SSM_Story/2025/lion-part2-theory/">Part II</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and Attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet <d-cite key="yang2024parallelizing"></d-cite> and GLA <d-cite key="yang2024gated"></d-cite>, as well as the SSD algorithm in Mamba2 <d-cite key="dao2024transformers"></d-cite>, a chunkwise parallel form of Full Linear Attention could be an effective solution. Additionally, in models like Hydra <d-cite key="hwang2025hydra"></d-cite>, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking Full Linear Attention, particularly for LION-S and LION-D, where the mask $\mathbf{M}$ structure is known? The aim of chunking Full Linear Attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does <strong>not</strong> require chunking during training allowing for higher throughput, especially for short sequences, when compared to other models such as Hydra <d-cite key="hwang2025hydra"></d-cite>. While in <strong>Gated Linear Attention (GLA)</strong> <d-cite key="yang2024gated"></d-cite>, <strong>DeltaNet</strong> <d-cite key="yang2024parallelizing"></d-cite>, and the <strong>SSD algorithm of Mamba2</strong> <d-cite key="dao2024transformers"></d-cite> <em>causal-specific</em> chunking methods are employed, we extend this to the non-causal case as well.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>The key idea of chunking is that instead of processing the entire sequence of length $L$, we divide it into $N$ subsequences of length $C$, where $N \times C = L$.<br/> To achieve this, we start with the Full Linear Attention formulation:</p> \[\mathbf{Y} = (\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}) \mathbf{V}\] <p>we first chunk the queries, keys and values into submatrices</p> \[\mathbf{Q}_{[i]} , \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\] <p>Now, given the form \((\mathbf{A} \odot \mathbf{M})\), where \(\mathbf{A} = \mathbf{Q} \mathbf{K}^\top\) we can construct the chunkwise form in four parts</p> <ul> <li>Chunkwise \(\mathbf{A}_{[ij]}\)</li> <li>Chunkwise form for the scaling matrix $\mathbf{C}_{[ij]}$</li> <li>The chunked hidden state to shape the unscaled output $\mathbf{S}_{[i(j-1)]}$</li> <li>Finally the output of the chunk $i$ which is $\mathbf{Y}_{[i]}$</li> </ul> <p>using these chunked matrices we shape the full linear atteniton in chunk form as bellow:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[i(j-1)]} + \text{Sum}(\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; = \mathbf{S}_{[i(j-1)]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]}, \\ \mathbf{Y}_{[i]} &amp; = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. And $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full maks $\mathbf{M}$ at chunk $ij$ like:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Let‚Äôs start with an example, chunking the Attention matrix $\mathbf{A}$ for a sequence of $L=9$ with $C=3$ chunk size in detail below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/att_chunk.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/att_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Chunking simply involves computing the queries and keys for each boxed sub-matrix, as illustrated for the upper, lower, and diagonal chunks. For every Attention matrix chunk $[ij]$, the computation follows the same pattern, multiplying the corresponding queries and keys for that chunk.</p> <p>But does the same approach apply to Selective and Fixed masks?</p> <p>In reality, chunking the Attention mask is slightly different and even more critical than chunking Attention itself due to its unique structure. Below, we provide a detailed explanation of how to chunk the Attention mask for LION-D and LION-S.</p> <p>üöÄ <strong>Note:</strong> The chunking visualization and details of this part are exclusively on the blogpost version.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <p>Let‚Äôs start with the decay mask, as it is simpler and easier to visualize. For LION-D, the final mask is a Toeplitz mask constructed using the scalar decay factor $\lambda$. We can visualize how the mask is structured.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/maskdec_chunk.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/maskdec_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The full mask of LION-D (or full RetNet mask) is constructed simply by the submatrix of $\Gamma$, which is a <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrix</a> itself. Regardless of where the chunk is located, whether in the upper or lower part of the mask matrix $\mathbf{M}$, it retains the same property of being a fraction of the Toeplitz matrix $\Gamma$ as bellow:</p> \[\mathbf{M}_{[ij]} = \Gamma \lambda^{|i-j|}\] <p>A pytorch implementation for LION-D Chunk Mask is provided below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_decay_partial</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">"</span><span class="s">ij</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">e</span>
    <span class="k">return</span> <span class="n">m</span>
</code></pre></div></div> <h3 id="lion-s-chunk">LION-S Chunk</h3> <p>The full mask of LION-S is more tricky than LION-D since the upper lower and the diagonal part of the mask are shaped differently:</p> <ul> <li>The <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Upper</span> part is influenced only by the decay factors applied from the end to the beginning of the sequence.</li> <li>The <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Diagonal</span> part incorporates contributions from both directions, spanning from the start to the end and from the end to the start.</li> <li>The <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Lower</span> part is influenced only by the decay factors applied from the beginning to the end of the sequence.</li> </ul> <p>Let‚Äôs visualize LION-S mask as well:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/masksel_chunk.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/masksel_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For example, the chunk [1,3] has only the cumulative decay factors multiplied from the beginning up to the last three sequence elements, while the chunk [3,1] has only the decay factors multiplied from the end up to the first three sequence elements. This is the reason for using the matrices $\mathbf{L}^F$ and $\mathbf{L}^B$ to compute the cumulative products of the decay factors, progressing from the beginning to the end of the sequence and in reverse which can be created simply by <code class="language-plaintext highlighter-rouge">L^F = cumprod(a)</code> and <code class="language-plaintext highlighter-rouge">L^B = cumprod(flip(a))</code>.</p> <h3 id="the-code-for-lion-s-chunk-mask">The code for LION-S Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)[</span>
            <span class="p">...,</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span> <span class="p">:</span> <span class="p">(</span><span class="n">chunk_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_length</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mask_backward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span>
        <span class="p">...,</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span> <span class="p">:</span> <span class="p">(</span><span class="n">chunk_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">,</span> <span class="p">:</span>
    <span class="p">]</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mask_selective_partial</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">a_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_forward</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">chunk_index</span><span class="p">,</span>
        <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">a_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_backward</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">chunk_index</span><span class="p">,</span>
        <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag_embed</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">-</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)),</span>
        <span class="n">offset</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)[...,</span> <span class="p">:</span> <span class="n">a_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">a_for</span> <span class="o">+</span> <span class="n">a_back</span> <span class="o">-</span> <span class="n">i</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">a_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now that we have all elements in place let‚Äôs see how these models are working in practice on real-world datasets for masked language modeling and image classification.</p> <h2 id="next-up">Next Up</h2> <p>In the <a href="/SSM_Story/2025/lion-part4-results/">final part of this series</a>, we present the advantages of using LION compared to other methods for training SSMs or Linear Transformers.</p> <p>We also present the trade-offs for different LION ü¶Å models and compare them with other well-known SSMs and Softmax Transformers.</p> <p><a href="/SSM_Story/2025/lion-part4-results/">Continue reading to Part IV - Results</a></p>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference]]></summary></entry><entry><title type="html">LION ü¶Å Part IV - Results</title><link href="https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results/" rel="alternate" type="text/html" title="LION ü¶Å Part IV - Results"/><published>2025-02-28T00:00:00+00:00</published><updated>2025-02-28T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results</id><content type="html" xml:base="https://arshiaafzal.github.io/SSM_Story/2025/lion-part4-results/"><![CDATA[<p>[<a href="https://www.arxiv.org/abs/2502.16249">Paper</a>] [<a href="https://github.com/LIONS-EPFL/LION">Code</a>]</p> <ol> <li><a href="/SSM_Story/2025/lion-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/SSM_Story/2025/lion-part2-theory/">Part II - Bi-directional RNN</a></li> <li><a href="/SSM_Story/2025/lion-part3-chunk/">Part III - Chunkwise Parallel from of LION</a></li> <li>Part IV - Results</li> </ol> <p>In the final part of our LION series, we will present and discuss a selection of experimental results across various domains, including vision tasks, masked language modeling (MLM), and different LION architectures. These results not only highlight LION‚Äôs versatility and efficiency across diverse applications but also serve as a preview of the comprehensive findings detailed in the full paper.</p> <h2 id="image-classification-performance-overview">Image Classification Performance Overview</h2> <h3 id="model-comparisons">Model Comparisons</h3> <p>We evaluated LION‚Äôs performance, efficiency, and training times against state-of-the-art SSMs and Transformers for image classification. The results demonstrate that LION achieves competitive performance while offering significant advantages in training speed and efficiency.</p> <table> <thead> <tr> <th>Model</th> <th>#Param</th> <th>Imagenet Top-1 Acc.</th> <th>Train. time</th> </tr> </thead> <tbody> <tr> <td>$\text{ViT}$</td> <td>86M</td> <td>$77.9$</td> <td>$\times 1$</td> </tr> <tr> <td>$\text{DeiT}$</td> <td>86M</td> <td>$\underline{81.8}$</td> <td>$\times 1$</td> </tr> <tr> <td>$\text{Hydra}$</td> <td>104M</td> <td>$81.0$</td> <td>$\times 2.51$</td> </tr> <tr> <td>$\text{Vim}$</td> <td>98M</td> <td>$\mathbf{81.9}$</td> <td>$\times 10.86$</td> </tr> <tr> <td>$\text{LION-}\text{üî•}$</td> <td>86M</td> <td>$74.7$</td> <td>$\mathbf{\times 0.73}$</td> </tr> <tr> <td>$\text{LION-D}$</td> <td>86M</td> <td>$77.8$</td> <td>$\times \underline{1.39}$</td> </tr> <tr> <td>$\text{LION-D}^{\natural}$</td> <td>86M</td> <td>$80.2$</td> <td>$\times 1.48$</td> </tr> <tr> <td>$\text{LION-S}$</td> <td>86M</td> <td>$76.3$</td> <td>$\times 1.46$</td> </tr> <tr> <td>$\text{LION-S}^{\natural}$</td> <td>86M</td> <td>$79.9$</td> <td>$\times 1.68$</td> </tr> </tbody> </table> <div class="caption" style="color: #666666; margin-top: 1px;"> Model performance comparison on ImageNet classification, showing parameter count, top-1 accuracy, and relative training time. </div> <p>As shown in the table above, LION models achieve competitive performance with vision-specific SSMs like Vim, while being significantly faster during training. LION-D performs comparably to Vim and surpasses Hydra <d-cite key="hwang2025hydra"></d-cite>, while training approximately 7x faster than Vim <d-cite key="zhu2024vision"></d-cite>. Notably, LION-üî• demonstrates the highest training speed across all models, showing that training with Full Linear Attention is significantly faster than chunkwise parallel training (used in Hydra) and considerably faster than the scan algorithm, even with optimized GPU kernels (as used in Vim). \(LION-S^{\natural}\) and \(LION-D^{\natural}\) modify the order of patches in an image to better capture the locality inherent in spatial patterns. By rearranging the patch sequence, these models enhance their understanding of local structures while still leveraging the efficiency of Linear Attention mechanisms similar to xLSTM <d-cite key="alkin2024vision"></d-cite>.</p> <h3 id="memory-efficiency">Memory Efficiency</h3> <p>The LION family demonstrates excellent memory efficiency across both vision and language tasks. Figure below shows inference memory usage with a batch size of 64 across different image resolutions, LION models (RNN form) maintain reasonable memory consumption even at high resolutions up to 2496 pixels, while adding minimal training overhead in BERT-style language modeling scenarios. In contrast, baseline models like ViT and DeiT run out of memory (OOM) at much lower resolutions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/fig1_plot.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/fig1_plot.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Memory Usage Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Memory usage during inference across different architectures with batch size 64. LION models (RNN form) maintain reasonable memory consumption at high resolutions while other models run out of memory.</figcaption> </figure> </div> </div> <h3 id="training-time-analysis">Training Time Analysis</h3> <p>The LION family demonstrates remarkable training efficiency across both vision and language tasks. As shown in the table below, LION variants add minimal training overhead compared to SSMs.</p> <table> <thead> <tr> <th>Task</th> <th><span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">LION-üî• </span></th> <th><span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">LION-D </span></th> <th><span style="background-color: rgb(255, 233, 211) ; padding: 3px; color:black">LION-S </span></th> <th>Hydra</th> <th>Vim</th> </tr> </thead> <tbody> <tr> <td>Vision</td> <td>$\times 0.73$</td> <td>$\times 1.39$</td> <td>$\times 1.46$</td> <td>$\times 2.51$</td> <td>$\times 10.86$</td> </tr> <tr> <td>MLM</td> <td>$\times 0.95$</td> <td>$\times 1.10$</td> <td>$\times 1.32$</td> <td>$\times 3.13$</td> <td>‚úó</td> </tr> </tbody> </table> <div class="caption" style="color: #666666; margin-top: 1px;"> Training Times (relative to Transformer) ‚Üì </div> <p>For vision tasks, LION-üî• achieves remarkable speed, training 27% faster than standard vision Transformers <d-cite key="dosovitskiy2020image"></d-cite>. Even the more complex LION variants maintain competitive training times, with LION-D and LION-S training only ~1.4x slower than Transformers. This is significantly better than competing approaches like Hydra (2.51x slower) and Vim (10.86x slower).</p> <p>In MLM tasks, the efficiency gains are even more pronounced. LION-üî• nearly matches Transformer training speed at just 0.95x, while LION-D adds only 10% overhead. Even LION-S remains efficient at 1.32x. All LION variants significantly outperform Hydra‚Äôs 3.13x slowdown, while Vim is not applicable to MLM tasks (marked as ‚úó).</p> <h2 id="mlm-results">MLM Results</h2> <p>For masked language modeling (MLM) tasks, we evaluated LION models against BERT <d-cite key="devlin2018bert"></d-cite> and Hydra on both MLM pretraining and GLUE benchmark finetuning. The results show that LION variants achieve competitive performance while maintaining good training efficiency.</p> <table> <thead> <tr> <th>Model</th> <th>MLM Acc.</th> <th>GLUE</th> <th>Train. time</th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>$\underline{69.88}$</td> <td>$\mathbf{82.95}$</td> <td>$\times 1$</td> </tr> <tr> <td>Hydra</td> <td>$\mathbf{71.18}$</td> <td>$\underline{81.77}$</td> <td>$\times 3.13$</td> </tr> <tr> <td><span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">LION-üî• </span></td> <td>$67.11$</td> <td>$80.76$</td> <td>$\times \mathbf{0.95}$</td> </tr> <tr> <td><span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">LION-D </span></td> <td>$68.64$</td> <td>$81.34$</td> <td>$\times \underline{1.10}$</td> </tr> <tr> <td><span style="background-color: rgb(255, 233, 211) ; padding: 3px; color:black">LION-S </span></td> <td>$69.16$</td> <td>$81.58$</td> <td>$\times 1.32$</td> </tr> </tbody> </table> <div class="caption" style="color: #666666; margin-top: 1px;"> C4 MLM and GLUE results for the LARGE scale (334M). For each dataset, the best and second best results are highlighted with bold and underline respectively. </div> <h2 id="lion-architecture-variants-and-trade-offs">LION Architecture Variants and Trade-offs</h2> <p>Let‚Äôs explore how different LION variants handle the trade-off between memory usage and inference speed. We will look at three key approaches:</p> <ol> <li>Full Linear Attention - The standard approach using the Full Attention matrix.</li> <li>Bidirectional RNN - Our memory-efficient RNN formulation.</li> <li>LION Chunk - A balanced approach using chunked computation.</li> </ol> <h3 id="memory-vs-speed-trade-offs">Memory vs Speed Trade-offs</h3> <p>The first plot below shows how these approaches compare in terms of memory efficiency and inference speed in LION-D. The RNN approach proves to be the most memory-efficient, while Full Attention uses the most memory. LION Chunk provides a nice middle ground - it uses less memory than Full Attention while actually achieving faster inference speeds than both alternatives. This makes it particularly attractive when you need to balance performance with resource constraints.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/fig3_plot.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/fig3_plot.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Impact of Chunk Size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Analysis of how chunk size affects model performance across different LION-D variants.</figcaption> </figure> </div> </div> <p>For LION-üî•, we see a similar pattern, but the chunking approach is even more pronounced.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/linear_chunking.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/linear_chunking.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Linear Chunking Analysis" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Evaluation of linear chunking strategies and their impact on model efficiency of LION-üî•.</figcaption> </figure> </div> </div> <p>Lastly for LION-S, we see that the chunking approach is only faster at lower resolutions - at higher resolutions, the overhead from mask calculations starts to slow it down.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/SSM_Story/assets/img/selective_chunking.svg" sizes="95vw"/> <img src="/SSM_Story/assets/img/selective_chunking.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Selective Chunking Analysis" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance comparison of selective chunking approaches across different sequence lengths with LION-S.</figcaption> </figure> </div> </div> <h2 id="future-directions">Future Directions</h2> <ul> <li> <p><strong>Expanding LION‚Äôs Potential:</strong> Our experiments focused on three main mask choices, but LION has the potential to accelerate other Linear Transformer variants for bidirectional tasks.</p> </li> <li> <p><strong>Optimizing Chunkwise Parallelism:</strong> The chunkwise parallel implementation during inference was done in PyTorch, with room for optimization through GPU kernel programming to reduce I/O overhead and improve speed.</p> </li> <li> <p><strong>Stabilizing Hydra and Mamba with LION:</strong> Hydra <d-cite key="hwang2025hydra"></d-cite> and Mamba <d-cite key="gu2023mamba"></d-cite> activations led to unstable training under Full Attention, suggesting LION could be used to stabilize these variants in the future.</p> </li> </ul> <h1 id="last-points">Last Points</h1> <p>We encourage the readers of this blog post to read the full <a href="https://www.arxiv.org/abs/2502.16249">paper</a> for more details about the LION framework and experimental setups. The implementation details are available in the <a href="https://github.com/LIONS-EPFL/LION">code repository</a>.</p> <p>If you use this work, please consider citing the paper:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">afzal2025linear</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Linear Attention for Efficient Bidirectional Sequence Modeling}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Afzal, Arshia and Abad Rocamora, Elias and Candogan, Leyla Naz and Puigdemont, Pol and Tonin, Francesco and Wu, Yongtao and Shoaran, Mahsa and Cevher, Volkan}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.16249}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2502.16249}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.48550/arXiv.2502.16249}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Comprehensive results on Vision, MLM and more LION variants]]></summary></entry></feed>